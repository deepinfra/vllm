from aioprometheus import Gauge, Counter

# The begin-* and end* here are used by the documentation generator
# to extract the metrics definitions.

# begin-metrics-definitions
gauge_avg_prompt_throughput = Gauge("vllm:avg_prompt_throughput_toks_per_s",
                                    "Average prefill throughput in tokens/s.")
gauge_avg_generation_throughput = Gauge(
    "vllm:avg_generation_throughput_toks_per_s",
    "Average generation throughput in tokens/s.")

gauge_scheduler_running = Gauge(
    "vllm:num_requests_running",
    "Number of requests that is currently running for inference.")
gauge_scheduler_swapped = Gauge("vllm:num_requests_swapped",
                                "Number requests swapped to CPU.")
gauge_scheduler_waiting = Gauge("vllm:num_requests_waiting",
                                "Number of requests waiting to be processed.")

gauge_gpu_cache_usage = Gauge(
    "vllm:gpu_cache_usage_perc",
    "GPU KV-cache usage. 1 means 100 percent usage.")
gauge_cpu_cache_usage = Gauge(
    "vllm:cpu_cache_usage_perc",
    "CPU KV-cache usage. 1 means 100 percent usage.")
gauge_max_gen_time_ms = Gauge("vllm:max_gen_time_ms",
                              "Longest time to step a batch in milliseconds.")
gauge_avg_gen_time_ms = Gauge("vllm:avg_gen_time_ms",
                              "Average time to step a batch in milliseconds.")
gauge_max_prompt_time_ms = Gauge(
    "vllm:max_prompt_time_ms",
    "Longest time to step a prompt prefetch in milliseconds.")
gauge_avg_prompt_time_ms = Gauge(
    "vllm:avg_prompt_time_ms",
    "Average time to step a prompt prefetch in milliseconds.")
total_num_requests = Counter(
    "vllm:total_num_requests",
    "Total number of requests processed by the server.")
total_input_tokens = Counter(
    "vllm:total_input_tokens",
    "Total number of input tokens processed by the server.")
total_output_tokens = Counter(
    "vllm:total_output_tokens",
    "Total number of output tokens generated by the server.")
# end-metrics-definitions

labels = {}


def add_global_metrics_labels(**kwargs):
    labels.update(kwargs)


def record_counter_metrics(num_requests: int, num_input_tokens: int,
                           num_output_tokens: int):
    total_num_requests.add(labels, num_requests)
    total_input_tokens.add(labels, num_input_tokens)
    total_output_tokens.add(labels, num_output_tokens)


def record_metrics(
    avg_prompt_throughput: float,
    avg_generation_throughput: float,
    scheduler_running: int,
    scheduler_swapped: int,
    scheduler_waiting: int,
    gpu_cache_usage: float,
    cpu_cache_usage: float,
    avg_gen_time_ms: float,
    max_gen_time_ms: float,
    avg_prompt_time_ms: float,
    max_prompt_time_ms: float,
):
    gauge_avg_prompt_throughput.set(labels, avg_prompt_throughput)
    gauge_avg_generation_throughput.set(labels, avg_generation_throughput)
    gauge_scheduler_running.set(labels, scheduler_running)
    gauge_scheduler_swapped.set(labels, scheduler_swapped)
    gauge_scheduler_waiting.set(labels, scheduler_waiting)
    gauge_gpu_cache_usage.set(labels, gpu_cache_usage)
    gauge_cpu_cache_usage.set(labels, cpu_cache_usage)
    gauge_avg_gen_time_ms.set(labels, avg_gen_time_ms)
    gauge_max_gen_time_ms.set(labels, max_gen_time_ms)
    gauge_avg_prompt_time_ms.set(labels, avg_prompt_time_ms)
    gauge_max_prompt_time_ms.set(labels, max_prompt_time_ms)
