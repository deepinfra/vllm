INFO 10-22 20:41:22 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config:
	model='deepseek-ai/DeepSeek-OCR'
	speculative_config=None
	tokenizer='deepseek-ai/DeepSeek-OCR'
	skip_tokenizer_init=False
	tokenizer_mode=auto
	revision=None
	override_neuron_config=None
	tokenizer_revision=None
	trust_remote_code=True
	dtype=torch.bfloat16
	max_seq_len=8192
	download_dir=None
	load_format=auto
	tensor_parallel_size=1
	pipeline_parallel_size=1
	disable_custom_all_reduce=False
	quantization=None
	enforce_eager=False
	kv_cache_dtype=auto
	device_config=cuda
	decoding_config=DecodingConfig(
		guided_decoding_backend='xgrammar'
		reasoning_backend=None
	)
	observability_config=ObservabilityConfig(
		show_hidden_metrics=False
		otlp_traces_endpoint=None
		collect_model_forward_time=False
		collect_model_execute_time=False
	)
	seed=None
	served_model_name=deepseek-ai/DeepSeek-OCR
	num_scheduler_steps=1
	multi_step_stream_outputs=True
	enable_prefix_caching=None
	chunked_prefill_enabled=False
	use_async_output_proc=True
	disable_mm_preprocessor_cache=False
	mm_processor_kwargs=None
	pooler_config=None
	compilation_config={
		"splitting_ops":[]
		"compile_sizes":[]
		"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1]
		"max_capture_size":256
	}
	use_cached_outputs=False
